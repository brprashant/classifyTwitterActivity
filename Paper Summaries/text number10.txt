Overview: Write a short paragraph summarizing the content of the paper.
The paper tries to build on existing knowledge of location analysis based on content, to fit a GMM model to the problem. They fit the model, and offer variations, and run the data through various tests to discuss the results.

Algorithm: Describe in more detail the primary algorithm proposed or applied in the paper.
from the data, they retrieve a set of n-grams  and fit it to a GMM model of co-ordinates of locations possible.
They further try try to improve accuracy, precision and calibration in the process by adding features to the model.
Weighting by quality included features discussed in section 4.2 . They carry forward GMM-Qpr-Convar-Sum-Prod.
Weighting by error involved in the training set and include it in the model by including a power of the error found. they traverse between 0.5 to 10 and decide on an exponential value of 4.
Weighing by optimization. They try the model using quantities in 4.2 and just one ngram from a feature.

Hypothesis: List the hypotheses the authors test in the paper (note that these are not always explicitly stated).
They try to identify concrete quantitate methods to infer locations of individuals in a social network by fitting them to a global co-ordinate model, in a precise and calibrated manner. 
They identify, quantify and calibrate the model and allow for variations in every step. 

Data: Describe the data used in the experiments
Twitter data.
From streaming API.
For on year from Jan 2012 - Jan 2013.
It is expected to be a percentage of all the data available in the API>
They have at their disposal ~13 million geotagged tweets from the time period. Amounting to 0.8 to 1.6 % of 1 % of all tweets from that time. 
They run various tests by training the model from day to day on current data, for various tests. 
>50 tests were run on the data set. 
They strip information from the user profile, geo location of tweet etc. 
They also calibrate the language available from the data.

Experiments: Briefly describe how are the experiments are organized.
Its almost as explained in the algorithm
They run varioutins of the training data based on when they train the data and when they use the data in the model.
All variations are scheduled and run in the available data to check for results. 

Results: Describe the results and their significance.
There are quantifiable parameters one can use using a GMM model to geographically locate a  node in a social network.
The tests based on the parameters considered has a rather successfulresult in analyzing geo locations. 
Some factors improve and some factors not necessarily so in the process of location.
Some interesting points

The GMM model is time in sensitive. 
The parameters involved in accuracy , precition and calibration have major effects in the model and results. 
Geo location from a user profile, is not necessarily helping in the training and analysis as it is any way close to being inferred from other parameters.
Training duration looses value after a particular time in helping the cause.
The n gram list is best when kept lower. 

Assumptions: List some of the important assumptions the authors make in their work.


Synthesis: Are there claims you disagree with? What would you do differently? What would you do next?

How extensible outside of the realm of geo location is this valuable? 
Not to undermine the geo location problem, but once the geo location is found, will this data still be needed, and if so how can it be pre analyses or involved in the process.
Would including image processing and its data into the model be a possible idea?
If this is extended to generic text being fit into a model built around a medical subset, would it help in reaching out to blogs/nodes who need help!? 


Related Papers: List 2-3 papers that are most similar to this paper. For each, briefly list how this paper is different.